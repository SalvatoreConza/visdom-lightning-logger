{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b921ca47f4594489a418a6567c676a15": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a3b401246fec46b1a3025f5c2bd01756",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 4/4  \u001b[38;2;98;6;224mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 938/938 \u001b[2m0:00:28 â€¢ 0:00:00\u001b[0m \u001b[2;4m33.27it/s\u001b[0m \u001b[3mv_num: 0.1\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 4/4  <span style=\"color: #6206e0; text-decoration-color: #6206e0\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> 938/938 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:28 â€¢ 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">33.27it/s</span> <span style=\"font-style: italic\">v_num: 0.1</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "a3b401246fec46b1a3025f5c2bd01756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi2AVbfmerQS",
        "outputId": "1e7cf5dd-9edf-4095-ce8f-609c4bf05621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.12/dist-packages (from visdom) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from visdom) (1.16.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from visdom) (2.32.4)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.12/dist-packages (from visdom) (6.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from visdom) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.12/dist-packages (from visdom) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.12/dist-packages (from visdom) (1.9.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from visdom) (3.6.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from visdom) (11.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->visdom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->visdom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->visdom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->visdom) (2026.1.4)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=d54f30aa0fb43b75aa13ed1695d00d0dceb18208c216e459562205365545bd96\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/6c/38/64eeaa310e325aacda723e6df1f79ab5e9f31ba195264e04a8\n",
            "Successfully built visdom\n",
            "Installing collected packages: visdom\n",
            "Successfully installed visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "#install visdom\n",
        "!pip install visdom"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start visdom server in background\n",
        "import subprocess\n",
        "subprocess.Popen([\"python\", \"-m\", \"visdom.server\", \"-port\", \"8097\"])\n",
        "import time; time.sleep(3)"
      ],
      "metadata": {
        "id": "tvdRWemLez_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# user interface visualization with grok\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# start visdom server in the background\n",
        "subprocess.Popen([\"python\", \"-m\", \"visdom.server\", \"-port\", \"8097\"])\n",
        "time.sleep(3)  # give it a few seconds to boot up\n",
        "\n",
        "# then connect ngrok\n",
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "ngrok.set_auth_token(\"36QylUhTI8bXFJ5rOZskDze1ka6_qw5dGXXyaeB4bjvvvSmS\")\n",
        "url = ngrok.connect(8097)\n",
        "print(f\"open visdom at: {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuC6azW0e4jN",
        "outputId": "d9a924f9-82c9-4127-daea-d7bb96e4417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "open visdom at: NgrokTunnel: \"https://unsatisfiable-yair-satiably.ngrok-free.dev\" -> \"http://localhost:8097\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B5jORtipqy5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative cell to run if grok doesnt start, there will be no visdom user interface\n",
        "from visdom import Visdom\n",
        "viz = Visdom(port=8097)\n",
        "print(\"connected to visdom:\", viz.check_connection())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6lYKEu3fNzH",
        "outputId": "c7c89139-002b-4e9c-e49d-c224d8ee1be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:visdom:Setting up a new session...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Visdom: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train a simple CNN on MNIST with Visdom logging (manual)"
      ],
      "metadata": {
        "id": "xUdZEnM7KMrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard imports ML and NN related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# import visdom for data visualizations\n",
        "from visdom import Visdom\n",
        "\n",
        "# simple CNN to build for task 1, with 2 convolutional layer, 1 fully connected layer and 1 output layer:\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "# forward pass defining the how dataas flows along the network:\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# connect to a visdom server on port 8097\n",
        "viz = Visdom(port=8097)\n",
        "# gpu if avaiable othervise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# move the weight to the device\n",
        "model = SimpleCNN().to(device)\n",
        "# chosing the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# vhoding the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# normalization for stable training\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "# load the training set (in batches)\n",
        "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "# load the evaluation set (not need to be in batches)\n",
        "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=1000)\n",
        "\n",
        "# train for full 10 full epochs\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device) # move data and target to the cpu or the gpu\n",
        "        optimizer.zero_grad() # clear gradient from previos batch\n",
        "        loss = criterion(model(data), target) # compute the loss beetween the data that made trough a forward pass and the target\n",
        "        loss.backward() # backpropagation\n",
        "        optimizer.step() # weights update\n",
        "        running_loss += loss.item() # loss accumulation\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader) # average loss acroass the batches\n",
        "    # as asked by task 1 I do a visdom manual logging.\n",
        "    # win='loss' identifies which plot window to update. update='append' adds to the existing line rather than replacing it. The opts dict just sets titles and labels.\n",
        "    viz.line(Y=[avg_loss], X=[epoch], win='loss', update='append',\n",
        "             opts=dict(title='training loss', xlabel='epoch', ylabel='loss'))\n",
        "\n",
        "    # switch to evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    # I disable gradient here because I dont need it here\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            correct += model(data).argmax(1).eq(target).sum().item()\n",
        "    acc = correct / len(test_loader.dataset)\n",
        "    # ANOTHER visdom manual logging for accuracy...\n",
        "    viz.line(Y=[acc], X=[epoch], win='acc', update='append',\n",
        "             opts=dict(title='accuracy', xlabel='epoch', ylabel='accuracy'))\n",
        "    # and ANOTHER visdom manual logging for learning rate...\n",
        "    viz.line(Y=[optimizer.param_groups[0]['lr']], X=[epoch], win='lr', update='append',\n",
        "             opts=dict(title='learning rate', xlabel='epoch', ylabel='LR'))\n",
        "\n",
        "    print(f\"epoch {epoch}: loss={avg_loss:.4f}, acc={acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CREzMyzufZBw",
        "outputId": "e213925f-6c9d-41f3-cbd5-1f000fb2f97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:visdom:Setting up a new session...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 19.2MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 523kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.83MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.79MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: loss=0.1163, acc=0.9864\n",
            "epoch 1: loss=0.0366, acc=0.9862\n",
            "epoch 2: loss=0.0226, acc=0.9892\n",
            "epoch 3: loss=0.0146, acc=0.9889\n",
            "epoch 4: loss=0.0124, acc=0.9902\n",
            "epoch 5: loss=0.0083, acc=0.9899\n",
            "epoch 6: loss=0.0084, acc=0.9887\n",
            "epoch 7: loss=0.0052, acc=0.9901\n",
            "epoch 8: loss=0.0067, acc=0.9871\n",
            "epoch 9: loss=0.0054, acc=0.9866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Install PyTorch Lightning and understand logger interface                                                                                                                                    "
      ],
      "metadata": {
        "id": "WJijJVKGXBit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install pytorch lightning as asked by task 2, this would help to handle the boilerplater discussed in the google docs\n",
        "!pip install lightning -q\n",
        "\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.loggers import Logger # it's a blueprint that defines what methods a logger must have\n",
        "import inspect # its important to inspect what is inside other class, specifically in Logger\n",
        "\n",
        "# inspect all the method inside the Logger, this is part of understand the logger interface for task 2\n",
        "for name, method in inspect.getmembers(Logger, predicate=inspect.isfunction):\n",
        "    if not name.startswith('_'):\n",
        "        sig = inspect.signature(method)\n",
        "        print(f\"{name}{sig}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCQmew3gwz5",
        "outputId": "06810db0-56f2-4d3f-830f-225de54215d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m853.6/853.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hafter_save_checkpoint(self, checkpoint_callback: lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint) -> None\n",
            "finalize(self, status: str) -> None\n",
            "log_graph(self, model: torch.nn.modules.module.Module, input_array: Optional[torch.Tensor] = None) -> None\n",
            "log_hyperparams(self, params: Union[dict[str, Any], argparse.Namespace], *args: Any, **kwargs: Any) -> None\n",
            "log_metrics(self, metrics: dict[str, float], step: Optional[int] = None) -> None\n",
            "save(self) -> None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Implement a basic custom Lightning logger   "
      ],
      "metadata": {
        "id": "kyeMTWgFXGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after the exploration of Logger lets try to do task 3 and implement a basic custom lightning logger\n",
        "from lightning.pytorch.loggers import Logger\n",
        "from visdom import Visdom\n",
        "from argparse import Namespace\n",
        "\n",
        "# custom lightning logger:\n",
        "class VisdomLogger(Logger):\n",
        "    def __init__(self, env=\"main\", port=8097):\n",
        "        super().__init__()\n",
        "        self._viz = Visdom(port=port, env=env)\n",
        "        self._env = env\n",
        "\n",
        "# from the output of the previous cell I understand that Lightning requires every logger to have a name...\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"VisdomLogger\"\n",
        "# ...and a version:\n",
        "    @property\n",
        "    def version(self):\n",
        "        return \"0.1\"\n",
        "\n",
        "    # function for the logging of the model settings (loss, accuracy and learning rate)\n",
        "    def log_hyperparams(self, params, *args, **kwargs):\n",
        "        if isinstance(params, Namespace):\n",
        "            params = vars(params)\n",
        "        text = \"<br>\".join(f\"<b>{k}</b>: {v}\" for k, v in params.items())\n",
        "        self._viz.text(text, win=\"hyperparams\", opts=dict(title=\"Hyperparameters\"))\n",
        "\n",
        "    # eveytime a metric get logged this runs and plot each one on visdom automatically\n",
        "    # notice how the viz.line command of two cells ago it inside a for now, so it get repeated\n",
        "    def log_metrics(self, metrics, step=None):\n",
        "        for key, value in metrics.items():\n",
        "            self._viz.line(\n",
        "                Y=[float(value)], X=[step or 0],\n",
        "                win=key, update='append',\n",
        "                opts=dict(title=key, xlabel='step', ylabel=key)\n",
        "            )\n",
        "\n",
        "\n",
        "# wrapping the model in a lightning module\n",
        "class MNISTModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = SimpleCNN()  # reuse the same CNN from task 1\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # as task 1 we defined how data flows through\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    # lightning handle for each batch zero_grad(), backward(), optimizer.step()\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        loss = self.criterion(self(x), y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    # as task 1 we definte the validation_step\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred = self(x).argmax(1)\n",
        "        acc = pred.eq(y).float().mean()\n",
        "        self.log(\"val_acc\", acc)  # one line, no viz.line() boilerplate\n",
        "\n",
        "    # as task 1 we define the optimizer\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "# create the logger, create the trainer and hand it to the logger\n",
        "logger = VisdomLogger(port=8097)\n",
        "trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
        "trainer.fit(\n",
        "    MNISTModel(),\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=test_loader\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519,
          "referenced_widgets": [
            "b921ca47f4594489a418a6567c676a15",
            "a3b401246fec46b1a3025f5c2bd01756"
          ]
        },
        "id": "0C0XoZZhnuu2",
        "outputId": "c6057f2d-47a0-46c8-da8e-e23a29577e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:visdom:Setting up a new session...\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
            "INFO: ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ model     â”‚ SimpleCNN        â”‚  1.2 M â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ criterion â”‚ CrossEntropyLoss â”‚      0 â”‚ train â”‚     0 â”‚\n",
              "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
              "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ model     â”‚ SimpleCNN        â”‚  1.2 M â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ criterion â”‚ CrossEntropyLoss â”‚      0 â”‚ train â”‚     0 â”‚\n",
              "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 1.2 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 1.2 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 4                                                                          \n",
              "\u001b[1mModules in train mode\u001b[0m: 6                                                                                           \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.2 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 1.2 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 4                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 6                                                                                           \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b921ca47f4594489a418a6567c676a15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)`\n",
              "is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)`\n",
              "is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Profile hook overhead in PyTorch                                                                                                                                                                                 \n"
      ],
      "metadata": {
        "id": "EukEf8FQXVmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# will our implementation with lightning for solving the boilerplater slow the training process to much? let's find out\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# we consider a big and deep model for make noticible the eventual slow down of implementation with lightning\n",
        "class BigModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(*[nn.Linear(512, 512) for _ in range(20)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BigModel().to(device)\n",
        "x = torch.randn(64, 512).to(device) # fake input data, I just need to feed something to the model, I am interested not in the output but to the benchmarks\n",
        "\n",
        "# warmup, I run the model 50 times before measuring anything bcyase the GPUs are lazy\n",
        "for _ in range(50):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# benchmark WITHOUT hooks, run 500 forward and backward passes and time how long it takes\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(500):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "torch.cuda.synchronize()\n",
        "baseline = time.perf_counter() - start\n",
        "\n",
        "handles = [] # store references to every hook\n",
        "hook_data = {} # dictionary where hooks will dump their data\n",
        "\n",
        "# loops through every layer in the model, name is the label, module is the actual layer object itself\n",
        "for name, module in model.named_modules():\n",
        "    def fwd_hook(mod, inp, out, n=name):\n",
        "        hook_data[n] = out.detach().norm().item() # save the norm (.norm()) of the output of the layer a plain pytoch number (.item())\n",
        "    def bwd_hook(mod, grad_in, grad_out, n=name):\n",
        "        if grad_out[0] is not None:\n",
        "            hook_data[f\"{n}_grad\"] = grad_out[0].detach().norm().item() # save the norm (.norm()) of the gradient (if produced) of the layer a plain pytoch number (.item())\n",
        "\n",
        "    # attaches both hooks to the layer\n",
        "    handles.append(module.register_forward_hook(fwd_hook))\n",
        "    handles.append(module.register_full_backward_hook(bwd_hook))\n",
        "\n",
        "# benchmark WITH hooks, run 500 forward and backward passes and time how long it takes\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(500):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "torch.cuda.synchronize()\n",
        "hooked = time.perf_counter() - start\n",
        "\n",
        "# remove all hooks, leaving hooks means they keep running forever and slowing everything else down:\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "# calculate percentage slowdown:\n",
        "perc_slodown = ((hooked - baseline) / baseline) * 100\n",
        "print(f\"baseline:   {baseline:.3f}s\")\n",
        "print(f\"with hooks: {hooked:.3f}s\")\n",
        "print(f\"percentage slowdown:   {perc_slodown:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvnthzW7u_iI",
        "outputId": "3568e9dd-37dc-4771-aec0-a08d5079cc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline:   2.952s\n",
            "with hooks: 8.904s\n",
            "percentage slowdown:   201.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the percentage slowdown is big, way to big. the culprit is probably .item that forces a GPU->CPU syncronzation on very single hook call"
      ],
      "metadata": {
        "id": "cFoFbmUteDFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one little difference with respect to the code from before is in no .item in fwd_hook() and bwd_hook()\n",
        "handles = []\n",
        "hook_data = {}\n",
        "for name, module in model.named_modules():\n",
        "    def fwd_hook(mod, inp, out, n=name):\n",
        "        hook_data[n] = out.detach().norm()  # stays on GPU, no sync\n",
        "    def bwd_hook(mod, grad_in, grad_out, n=name):\n",
        "        if grad_out[0] is not None:\n",
        "            hook_data[f\"{n}_grad\"] = grad_out[0].detach().norm()  # no .item()\n",
        "    handles.append(module.register_forward_hook(fwd_hook))\n",
        "    handles.append(module.register_full_backward_hook(bwd_hook))\n",
        "\n",
        "# Benchmark optimized hooks\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(500):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "torch.cuda.synchronize()\n",
        "hooked_v2 = time.perf_counter() - start\n",
        "\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "perc_slodown_v2 = ((hooked_v2 - baseline) / baseline) * 100\n",
        "print(f\"baseline:          {baseline:.3f}s\")\n",
        "print(f\"hooks (.item()):   {hooked:.3f}s  â†’ {162.2:.1f}% overhead\")\n",
        "print(f\"hooks (no .item()): {hooked_v2:.3f}s  â†’ {perc_slodown_v2:.1f}% overhead\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQXyQsKGvz5X",
        "outputId": "3d313db9-8653-45a4-a9d4-a6ed4680d09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline:          2.952s\n",
            "hooks (.item()):   8.904s  â†’ 162.2% overhead\n",
            "hooks (no .item()): 4.200s  â†’ 42.3% overhead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "still high, probably the .norm() computation itself is adding to much computation too, it launches a gpu kernel in every hook. Let's try 3 experiments to figure out where the slowdown actually comes from."
      ],
      "metadata": {
        "id": "JWmPbxqQeXsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test 1: let's measure the slowdown of just having empty hooks, fwd_hook() and bwd_hook() have just a pass, means empty\n",
        "handles = []\n",
        "for name, module in model.named_modules():\n",
        "    def fwd_hook(mod, inp, out):\n",
        "        pass\n",
        "    def bwd_hook(mod, grad_in, grad_out):\n",
        "        pass\n",
        "    handles.append(module.register_forward_hook(fwd_hook))\n",
        "    handles.append(module.register_full_backward_hook(bwd_hook))\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(500):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "torch.cuda.synchronize()\n",
        "empty_hooks = time.perf_counter() - start\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "# test 2: let's measure the slowdown of giving things to the hooks, but we dont have computations here so no .norm()\n",
        "handles = []\n",
        "hook_data = {}\n",
        "for name, module in model.named_modules():\n",
        "    def fwd_hook(mod, inp, out, n=name):\n",
        "        hook_data[n] = out.detach()  # just store, no norm\n",
        "    def bwd_hook(mod, grad_in, grad_out, n=name):\n",
        "        if grad_out[0] is not None:\n",
        "            hook_data[f\"{n}_grad\"] = grad_out[0].detach()\n",
        "    handles.append(module.register_forward_hook(fwd_hook))\n",
        "    handles.append(module.register_full_backward_hook(bwd_hook))\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(500):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "torch.cuda.synchronize()\n",
        "store_hooks = time.perf_counter() - start\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "# test 3: let's measure now the slowdown of the compotutations, but in a realistic case,\n",
        "# so in the case in which we compute norm only every 100 steps: in fact, when we\n",
        "# monitoring, we dont need to log every single step\n",
        "handles = []\n",
        "hook_data = {}\n",
        "step = [0]\n",
        "for name, module in model.named_modules():\n",
        "    def fwd_hook(mod, inp, out, n=name):\n",
        "        if step[0] % 100 == 0:\n",
        "            hook_data[n] = out.detach().norm()\n",
        "    def bwd_hook(mod, grad_in, grad_out, n=name):\n",
        "        if step[0] % 100 == 0 and grad_out[0] is not None:\n",
        "            hook_data[f\"{n}_grad\"] = grad_out[0].detach().norm()\n",
        "    handles.append(module.register_forward_hook(fwd_hook))\n",
        "    handles.append(module.register_full_backward_hook(bwd_hook))\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(500):\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "    step[0] += 1\n",
        "torch.cuda.synchronize()\n",
        "sampled_hooks = time.perf_counter() - start\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "print(f\"baseline:              {baseline:.3f}s\")\n",
        "print(f\"slowdown with empty hooks:           {empty_hooks:.3f}s  â†’ {((empty_hooks-baseline)/baseline)*100:.1f}%\")\n",
        "print(f\"slowdown with store ref only:        {store_hooks:.3f}s  â†’ {((store_hooks-baseline)/baseline)*100:.1f}%\")\n",
        "print(f\"slowdown with norm every 100 steps:  {sampled_hooks:.3f}s  â†’ {((sampled_hooks-baseline)/baseline)*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuklAeByywo7",
        "outputId": "eaf5268e-722c-4e05-c664-a5eed0e72258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline:              2.952s\n",
            "slowdown with empty hooks:           3.371s  â†’ 14.2%\n",
            "slowdown with store ref only:        3.854s  â†’ 30.5%\n",
            "slowdown with norm every 100 steps:  3.099s  â†’ 5.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the biggest slowdown increase was caused by empty hooks, this is slowdown is high because the model is small and fast, on a real model with heavier layers that ratio shrinks drammatically so we are good."
      ],
      "metadata": {
        "id": "zU1yRvKIet9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. Complete starter task: \"Add a gradient norm logger to Visdom client\"       "
      ],
      "metadata": {
        "id": "ngf8ZCDYfH4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from visdom import Visdom\n",
        "import torch\n",
        "\n",
        "# define the gradient norm logger\n",
        "class GradientNormLogger:\n",
        "    def __init__(self, model, viz=None, port=8097, log_every=50):\n",
        "        self.viz = viz or Visdom(port=port) # viz connection\n",
        "        self.model = model\n",
        "        self.step = 0 # tracking the trainining steps\n",
        "        self.log_every = log_every # log every tot step not every single one\n",
        "        self._handles = [] # stores hook references\n",
        "        self._attach(model) # install hooks\n",
        "\n",
        "    def _attach(self, model):\n",
        "        # loop through every individual weight/bias in the model\n",
        "        for name, param in model.named_parameters():\n",
        "            # skip frozen/non-trainable params:\n",
        "            if param.requires_grad:\n",
        "                def hook(grad, layer_name=name):\n",
        "                    if self.step % self.log_every == 0: # only do work every tot steps\n",
        "                        norm = grad.detach().norm(2).item() # L2 norm will summarize how much large is this gradient\n",
        "                        # plot it on visdom:\n",
        "                        self.viz.line(\n",
        "                            Y=[norm], X=[self.step],\n",
        "                            win=f\"grad_norm/{layer_name}\",\n",
        "                            update='append',\n",
        "                            opts=dict(title=f\"grad norm: {layer_name}\",\n",
        "                                      xlabel=\"step\", ylabel=\"L2 norm\")\n",
        "                        )\n",
        "                # actually install the hook on this parameter\n",
        "                self._handles.append(param.register_hook(hook))\n",
        "\n",
        "    def on_step(self):\n",
        "        if self.step % self.log_every == 0:\n",
        "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                self.model.parameters(), float('inf'))\n",
        "            self.viz.line(\n",
        "                Y=[total_norm.item()], X=[self.step],\n",
        "                win=\"grad_norm/total\", update='append',\n",
        "                opts=dict(title=\"total gradient norm\",\n",
        "                          xlabel=\"step\", ylabel=\"L2 norm\")\n",
        "            )\n",
        "        self.step += 1\n",
        "\n",
        "    def detach(self):\n",
        "        # remove all hooks, leaving hooks means they keep running forever and slowing everything else down:\n",
        "        for h in self._handles:\n",
        "            h.remove()\n",
        "        self._handles.clear()\n",
        "\n",
        "\n",
        "# test it with a quick training run\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# installs all the hooks on the model\n",
        "grad_logger = GradientNormLogger(model, log_every=50)\n",
        "\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(data), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        grad_logger.on_step()\n",
        "    print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n",
        "\n",
        "# always clean up hooks when done\n",
        "grad_logger.detach()\n",
        "print(\"ok, all gradient norms were logged to visdom\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDZAdhM-y79-",
        "outputId": "7dce4560-ee1f-478e-9895-f7fdbf898a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:visdom:Setting up a new session...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss=0.0047\n",
            "Epoch 1: loss=0.0281\n",
            "ok, all gradient norms were logged to visdom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NcKts3bkv9AX"
      }
    }
  ]
}